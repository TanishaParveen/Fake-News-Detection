{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNge+CKVAhI/y2/A4d3+6xW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"id":"JGGMMhUlRNxJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737267537383,"user_tz":-330,"elapsed":4136,"user":{"displayName":"Tanisha Parveen","userId":"01592528713957342861"}},"outputId":"77bba8b1-7f7a-4b61-e7b1-8ed6982d898b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opendatasets in /usr/local/lib/python3.11/dist-packages (0.1.22)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.6.17)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.1.8)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2024.12.14)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.3.0)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.10)\n","Skipping, found downloaded files in \"./fake-news-detection\" (use force=True to force download)\n","                                               title  \\\n","0   Donald Trump Sends Out Embarrassing New Year’...   \n","1   Drunk Bragging Trump Staffer Started Russian ...   \n","2   Sheriff David Clarke Becomes An Internet Joke...   \n","3   Trump Is So Obsessed He Even Has Obama’s Name...   \n","4   Pope Francis Just Called Out Donald Trump Dur...   \n","\n","                                                text subject  \\\n","0  Donald Trump just couldn t wish all Americans ...    News   \n","1  House Intelligence Committee Chairman Devin Nu...    News   \n","2  On Friday, it was revealed that former Milwauk...    News   \n","3  On Christmas day, Donald Trump announced that ...    News   \n","4  Pope Francis used his annual Christmas Day mes...    News   \n","\n","                date  \n","0  December 31, 2017  \n","1  December 31, 2017  \n","2  December 30, 2017  \n","3  December 29, 2017  \n","4  December 25, 2017  \n"]}],"source":[" # Install the opendatasets package\n","!pip install opendatasets\n","\n","import opendatasets as od\n","import pandas as pd\n","\n","# Download the dataset\n","od.download(\"https://www.kaggle.com/datasets/bhavikjikadara/fake-news-detection\")\n","\n","# Load the dataset into a DataFrame\n","df = pd.read_csv('fake-news-detection/fake.csv')\n","print(df.head())"]},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install nltk\n","\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","# Function to preprocess text\n","def preprocess_text(text):\n","    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n","    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n","    text = text.lower()  # Convert to lower case\n","    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n","    return text\n","\n","# Example usage\n","sample_text = \"This is an example of a news article! It's full of misinformation.\"\n","clean_text = preprocess_text(sample_text)\n","print(clean_text)"],"metadata":{"id":"XmXQCkH-T-Dx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737267540991,"user_tz":-330,"elapsed":3615,"user":{"displayName":"Tanisha Parveen","userId":"01592528713957342861"}},"outputId":"c96f4dea-32e7-48c1-918a-4a124465a60d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","example news article full misinformation\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install scikit-learn\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Function to extract features using TF-IDF\n","def extract_features(texts):\n","    vectorizer = TfidfVectorizer(max_features=5000)\n","    features = vectorizer.fit_transform(texts)\n","    return features, vectorizer\n","\n","# Example usage\n","texts = [clean_text, \"Another example of a news article.\"]\n","features, vectorizer = extract_features(texts)\n","print(features.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-yBMqLkzZBVr","executionInfo":{"status":"ok","timestamp":1737267544125,"user_tz":-330,"elapsed":3141,"user":{"displayName":"Tanisha Parveen","userId":"01592528713957342861"}},"outputId":"f54174de-bf71-463b-8fec-d70c80e61254"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","[[0.         0.37930349 0.37930349 0.53309782 0.53309782 0.37930349\n","  0.        ]\n"," [0.53309782 0.37930349 0.37930349 0.         0.         0.37930349\n","  0.53309782]]\n"]}]},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install scikit-learn\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","# Function to train a model\n","def train_model(features, labels):\n","    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n","    model = LogisticRegression()\n","    model.fit(X_train, y_train)\n","    predictions = model.predict(X_test)\n","    accuracy = accuracy_score(y_test, predictions)\n","    return model, accuracy\n","\n","# Example usage with a mock dataset\n","features = np.array([[0.1, 0.2], [0.2, 0.1], [0.3, 0.4], [0.4, 0.3], [0.5, 0.6], [0.6, 0.5]])\n","labels = np.array([0, 1, 0, 1, 0, 1])  # 1 for fake, 0 for real\n","\n","model, accuracy = train_model(features, labels)\n","print(f'Model accuracy: {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CfnZcwluZISQ","executionInfo":{"status":"ok","timestamp":1737267546679,"user_tz":-330,"elapsed":2563,"user":{"displayName":"Tanisha Parveen","userId":"01592528713957342861"}},"outputId":"56f95717-a062-4254-fbb8-4bb13fa41e75"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Model accuracy: 1.0\n"]}]},{"cell_type":"code","source":["# Define a list of trusted sources\n","trusted_sources = ['cnn.com', 'bbc.com', 'nytimes.com']\n","\n","# Function to verify source credibility\n","def verify_source(source_url):\n","    for trusted in trusted_sources:\n","        if trusted in source_url:\n","            return True\n","    return False\n","\n","# Example usage\n","source_url = 'https://cnn.com/news/article'\n","is_trusted = verify_source(source_url)\n","print(f'Source trusted: {is_trusted}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VgU07t-_ZXH5","executionInfo":{"status":"ok","timestamp":1737267546679,"user_tz":-330,"elapsed":8,"user":{"displayName":"Tanisha Parveen","userId":"01592528713957342861"}},"outputId":"a4fceef5-118d-4879-d500-fd06b71f3ab4"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Source trusted: True\n"]}]},{"cell_type":"code","source":["vectorizer_path = '/path/to/your/vectorizer.pkl'\n","model_path = '/path/to/your/model.pkl'\n","# Import libraries\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","import pickle\n","\n","# Example dataset\n","texts = [\"This is fake news\", \"This is real news\"]\n","labels = [1, 0]  # 1 = Fake, 0 = Real\n","\n","# Train vectorizer and model\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(texts)\n","model = LogisticRegression()\n","model.fit(X, labels)\n","\n","# Save the vectorizer and model\n","with open('vectorizer.pkl', 'wb') as vec_file:\n","    pickle.dump(vectorizer, vec_file)\n","\n","with open('model.pkl', 'wb') as model_file:\n","    pickle.dump(model, model_file)\n"],"metadata":{"id":"C4C2yDPsZlAd","executionInfo":{"status":"ok","timestamp":1737267546679,"user_tz":-330,"elapsed":7,"user":{"displayName":"Tanisha Parveen","userId":"01592528713957342861"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import re\n","import pickle\n","\n","# Function to load the vectorizer and model\n","def load_model_and_vectorizer(vectorizer_path='vectorizer.pkl', model_path='model.pkl'):\n","    with open(vectorizer_path, 'rb') as vec_file:\n","        vectorizer = pickle.load(vec_file)\n","    with open(model_path, 'rb') as model_file:\n","        model = pickle.load(model_file)\n","    return vectorizer, model\n","\n","# Function to clean text\n","def clean_text(text):\n","    text = re.sub(r'\\W', ' ', text)\n","    text = re.sub(r'\\s+', ' ', text)\n","    text = text.lower()\n","    return text\n","\n","# Function to detect fake news\n","def detect_fake_news(article_text):\n","    vectorizer, model = load_model_and_vectorizer()\n","    clean_text_data = clean_text(article_text)\n","    features = vectorizer.transform([clean_text_data])  # Ensure same vectorizer is used\n","    prediction = model.predict(features)[0]\n","    is_fake = prediction == 1\n","\n","    return {\n","        'text': article_text,\n","        'is_fake': is_fake,\n","    }\n","\n","# Example usage\n","example_text = \"This is a sample news article to test the detection system.\"\n","result = detect_fake_news(example_text)\n","print(result)\n"],"metadata":{"id":"hRUVbVRwcDtf","executionInfo":{"status":"ok","timestamp":1737267546679,"user_tz":-330,"elapsed":6,"user":{"displayName":"Tanisha Parveen","userId":"01592528713957342861"}},"outputId":"3b1242c9-3cac-4371-b4da-187c5107b4e6","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["{'text': 'This is a sample news article to test the detection system.', 'is_fake': False}\n"]}]}]}